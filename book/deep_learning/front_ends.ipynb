{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# フロントエンド層\n",
    "\n",
    "フロントエンド（Front-end）層は、DNNにおいて入力データ（波形やスペクトログラムなど）を処理する最初の層です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D畳み込み層"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その名の通り、1次元方向に畳み込みを行う層です。波形データをスペクトログラムに変換する代わりに、「学習可能（learnable）な」特徴抽出器と見なした畳み込み層に入力することで、よりタスクに適した特徴量を得る、という動機で使われます。\n",
    "\n",
    "音楽音源分離モデルのフロントエンド層として多用されています[^LM19] [^demucs19]。\n",
    "\n",
    "[^LM19]: Yi Luo and Nima Mesgarani. Conv-tasnet: surpassing ideal time-frequency magnitude masking for speech separation. IEEE ACM Trans. Audio Speech Lang. Process., 27(8):1256–1266, 2019. URL: https://doi.org/10.1109/TASLP.2019.2915167.\n",
    "\n",
    "[^demucs19]: Alexandre Défossez, Nicolas Usunier, Léon Bottou, and Francis R. Bach. Demucs: deep extractor for music sources with extra unlabeled data remixed. CoRR, 2019. URL: http://arxiv.org/abs/1909.01174."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力： torch.Size([1, 2, 3120768])\n",
      "出力： torch.Size([1, 64, 3120768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "wav, sr = torchaudio.load(\"assets/example1.ogg\")\n",
    "wav = wav.unsqueeze(0)\n",
    "print('入力：', wav.shape)\n",
    "\n",
    "conv_1d = nn.Conv1d(\n",
    "    in_channels=2,\n",
    "    out_channels=64,\n",
    "    kernel_size=5,\n",
    "    stride=1,\n",
    "    padding=2,  # stride=1, padding=(kernel_size - 1) // 2　にすれば、出力サイズが入力サイズと同じになる\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    ")\n",
    "out = conv_1d(wav)\n",
    "print('出力：', out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スペクトログラムの周波数次元を「チャンネル次元」と見なせば、スペクトログラムを入力とすることも可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilated-Conv-1D \n",
    "\n",
    "初めて「サンプル単位の逐次音声合成」という力技に挑戦したWaveNet[^wavenet16]にて提案された畳み込み層です。畳み込みカーネルを「飛ばし飛ばし」に適用することで、受容野（receptive field）をなるべく広げることが目的で、大域的な構造を考慮するようなモデルに使われます。\n",
    "\n",
    "サンプル単位の音声合成は流石に効率が悪すぎましたが、Dilated-Conv-1D層を重ねたTCN（Temporal Convolutional Network）は音楽解析タスクで多用されています。\n",
    "\n",
    "[^wavenet16]: Aäron van den Oord et al., Wavenet: A generative model for raw audio. In Alan W. Black, editor, The 9th ISCA Speech Synthesis Workshop, SSW 2016, Sunnyvale, CA, USA, September 13-15, 2016, 125. ISCA, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力： torch.Size([1, 64, 3120768])\n"
     ]
    }
   ],
   "source": [
    "dil_conv_1d = nn.Conv1d(\n",
    "    in_channels=2,\n",
    "    out_channels=64,\n",
    "    kernel_size=5,\n",
    "    stride=1,\n",
    "    padding=\"same\",     # \"same\"に設定すれば、入出力長が等しくなるようにpaddingを自動で設定してくれる\n",
    "    dilation=8,         # dilation>1 だとdilated convolution\n",
    "    groups=1,\n",
    ")\n",
    "out = dil_conv_1d(wav) \n",
    "print('出力：', out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメトリックフロントエンド\n",
    "\n",
    "ニューラルネットのパラメーターは、ベクトルや配列だけでなく、「パラメーター付きの関数」で表すこともできます。\n",
    "パラメーターの値に制約を付けることで、より解釈しやすい処理になることが期待されています。\n",
    "\n",
    "その初の試みとして、**SincNet**[^RB18]は畳み込みカーネルを「SinC関数」で表す畳み込み層を提案しました。\n",
    "SincNetは、畳み込みカーネル：$\\omega_n, n\\in\\{0, ..., N-1\\}$を、学習可能なパラメーター$[f_1, f_2]$からなる関数で表します。\n",
    "\n",
    "$$\n",
    "\\omega_n^{f_1, f_2}=2f_2 sinc(2\\pi f_2n)-2f_1 sinc(2\\pi f_1n)\n",
    "$$\n",
    "\n",
    "$$\n",
    "sinc(x) = \\frac{sin(x)}{x}\n",
    "$$\n",
    "\n",
    "このカーネルを用いた畳み込み処理は「カットオフ周波数が$[f_1, f_2]$のバンドパスフィルタ」に相当します。\n",
    "\n",
    "畳み込み層が話者識別タスクにとって重要な帯域を自動的に学習するため、SincNetは一般的な畳み込みニューラルネットワークよりも高い精度を達成したほか、学習の収束速度も大きく向上したとのことです。\n",
    "\n",
    "\n",
    "[^RB18]: Mirco Ravanelli and Yoshua Bengio. Speaker recognition from raw waveform with sincnet. In 2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018, 1021–1028. IEEE, 2018. URL: https://doi.org/10.1109/SLT.2018.8639585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 80, 16000])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source code borrowed from: \n",
    "https://geoffroypeeters.github.io/deeplearning-101-audiomir_book/bricks_frontend.html\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SincConv_fast(nn.Module):\n",
    "    \"\"\"Sinc-based convolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : `int`\n",
    "        Number of input channels. Must be 1.\n",
    "    out_channels : `int`\n",
    "        Number of filters.\n",
    "    kernel_size : `int`\n",
    "        Filter length.\n",
    "    sample_rate : `int`, optional\n",
    "        Sample rate. Defaults to 16000.\n",
    "    Usage\n",
    "    -----\n",
    "    See `torch.nn.Conv1d`\n",
    "    Reference\n",
    "    ---------\n",
    "    Mirco Ravanelli, Yoshua Bengio,\n",
    "    \"Speaker Recognition from raw waveform with SincNet\".\n",
    "    https://arxiv.org/abs/1808.00158\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\n",
    "\n",
    "        super(SincConv_fast,self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "            #msg = (f'SincConv only support one input channel '\n",
    "            #       f'(here, in_channels = {in_channels:d}).')\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
    "        if kernel_size%2==0: self.kernel_size=self.kernel_size+1\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if bias: raise ValueError('SincConv does not support bias.')\n",
    "        if groups > 1: raise ValueError('SincConv does not support groups.')\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\n",
    "        low_hz = 30\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "        mel_v = np.linspace(self.to_mel(low_hz), self.to_mel(high_hz), self.out_channels + 1)\n",
    "        hz_v = self.to_hz(mel_v)\n",
    "\n",
    "\n",
    "        # filter lower frequency (out_channels, 1)\n",
    "        self.low_hz_v_ = nn.Parameter(torch.Tensor(hz_v[:-1]).view(-1, 1))\n",
    "\n",
    "        # filter frequency band (out_channels, 1)\n",
    "        self.band_hz_v_ = nn.Parameter(torch.Tensor(np.diff(hz_v)).view(-1, 1))\n",
    "\n",
    "        # Hamming window\n",
    "        #self.window_ = torch.hamming_window(self.kernel_size)\n",
    "        n_lin = torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n",
    "        self.window_ = 0.54-0.46*torch.cos(2*np.pi*n_lin/self.kernel_size);\n",
    "\n",
    "\n",
    "        # (1, kernel_size/2)\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.n_ = 2*np.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n",
    "            Batch of waveforms.\n",
    "        Returns\n",
    "        -------\n",
    "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n",
    "            Batch of sinc filters activations.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_ = self.n_.to(waveforms.device)\n",
    "        self.window_ = self.window_.to(waveforms.device)\n",
    "\n",
    "        low_v = self.min_low_hz  + torch.abs(self.low_hz_v_)\n",
    "        high_v = torch.clamp(low_v + self.min_band_hz + torch.abs(self.band_hz_v_),\n",
    "                           self.min_low_hz,\n",
    "                           self.sample_rate/2)\n",
    "        band_v = (high_v - low_v)[:,0]\n",
    "\n",
    "        f_times_t_low = torch.matmul(low_v, self.n_)\n",
    "        f_times_t_high = torch.matmul(high_v, self.n_)\n",
    "\n",
    "        band_pass_left = ((torch.sin(f_times_t_high) - torch.sin(f_times_t_low)) / (self.n_/2)) * self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations.\n",
    "        band_pass_center = 2 * band_v.view(-1,1)\n",
    "        band_pass_right= torch.flip(band_pass_left, dims=[1])\n",
    "\n",
    "        band_pass=torch.cat([band_pass_left,\n",
    "                             band_pass_center,\n",
    "                             band_pass_right],dim=1)\n",
    "\n",
    "        band_pass = band_pass / (2*band_v[:,None])\n",
    "\n",
    "        self.filters = (band_pass).view(self.out_channels, 1, self.kernel_size)\n",
    "\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride, padding=self.padding, dilation=self.dilation, bias=None, groups=1)\n",
    "\n",
    "\n",
    "model = SincConv_fast(out_channels=80, kernel_size=251, padding=125, sample_rate=16000, in_channels=1)\n",
    "X = torch.randn(2, 1, 16000)\n",
    "Y = model(X)\n",
    "print(Y.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
